{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60caf0dd",
   "metadata": {},
   "source": [
    "导入库//"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c90c3aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import copy\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ff2dc3",
   "metadata": {},
   "source": [
    "随机种子//"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b94ac4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90e2782",
   "metadata": {},
   "source": [
    "数据集//\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00dca041",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFARData:\n",
    "    def __init__(self, dataset='cifar10', calib_size=1000, test_size=1000, batch_size=128):\n",
    "        self.dataset = dataset.lower()\n",
    "        self.calib_size = calib_size\n",
    "        self.test_size = test_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_classes = 10 if dataset == 'cifar10' else 100\n",
    "        self._load_data()\n",
    "    \n",
    "    def _load_data(self):\n",
    "        transform_train = transforms.Compose([\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "        ])\n",
    "\n",
    "        transform_test = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "        ])\n",
    "        \n",
    "        if self.dataset == 'cifar10':\n",
    "            full_train_set = torchvision.datasets.CIFAR10(\n",
    "                root='./data', train=True, download=True, transform=transform_train)\n",
    "            test_set = torchvision.datasets.CIFAR10(\n",
    "                root='./data', train=False, download=True, transform=transform_test)\n",
    "        else:  # cifar100\n",
    "            full_train_set = torchvision.datasets.CIFAR100(\n",
    "                root='./data', train=True, download=True, transform=transform_train)\n",
    "            test_set = torchvision.datasets.CIFAR100(\n",
    "                root='./data', train=False, download=True, transform=transform_test)\n",
    "        \n",
    "        indices = np.arange(len(full_train_set))\n",
    "        labels = full_train_set.targets\n",
    "        \n",
    "        train_idx, calib_idx = train_test_split(\n",
    "            indices, test_size=self.calib_size, stratify=labels\n",
    "        )\n",
    "\n",
    "        self.split_indices = {\n",
    "            'train_idx': train_idx,\n",
    "            'calib_idx': calib_idx,\n",
    "        }\n",
    "        \n",
    "        self.train_set = Subset(full_train_set, train_idx)\n",
    "        self.calib_set = Subset(full_train_set, calib_idx)\n",
    "        self.test_set = test_set\n",
    "        \n",
    "        self.train_loader = DataLoader(\n",
    "            self.train_set, \n",
    "            batch_size=self.batch_size, shuffle=True, num_workers=2\n",
    "        )\n",
    "        \n",
    "        self.calib_loader = DataLoader(\n",
    "            self.calib_set, \n",
    "            batch_size=self.batch_size, shuffle=False, num_workers=2\n",
    "        )\n",
    "        \n",
    "        self.test_loader = DataLoader(\n",
    "            self.test_set, \n",
    "            batch_size=self.batch_size, shuffle=False, num_workers=2\n",
    "        )\n",
    "        \n",
    "        print(f\"Dataset: {self.dataset}\")\n",
    "        print()\n",
    "        print(f\"Dataset sizes for {self.dataset.upper()}:\")\n",
    "        print(f\"  Training set: {len(self.train_set)} samples\")\n",
    "        print(f\"  Calibration set: {len(self.calib_set)} samples\")\n",
    "        print(f\"  Test set: {len(self.test_set)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67734a67",
   "metadata": {},
   "source": [
    "残差块//"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff551214",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = nn.ReLU()(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = nn.ReLU()(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e3e714",
   "metadata": {},
   "source": [
    "Resnet//"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "902a8cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = nn.ReLU()(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = nn.AdaptiveAvgPool2d((1, 1))(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "def ResNet18(num_classes=10):\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes=num_classes)\n",
    "\n",
    "def ResNet34(num_classes=10):\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3], num_classes=num_classes)\n",
    "\n",
    "def ResNet50(num_classes=10):\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3], num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7163f03",
   "metadata": {},
   "source": [
    "训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43b138e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, data_loader, num_classes=10, lr=0.1, epochs=100):\n",
    "        self.model = model\n",
    "        if self.model == 'resnet18':\n",
    "            self.lr = 0.1\n",
    "        elif self.model == 'resnet34':\n",
    "            self.lr = 0.05\n",
    "        else:\n",
    "            self.lr = 0.03\n",
    "        self.train_loader = data_loader.train_loader\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.SGD(\n",
    "            model.parameters(), lr=self.lr, momentum=0.9, weight_decay=5e-4\n",
    "        )\n",
    "        self.scheduler = optim.lr_scheduler.MultiStepLR(\n",
    "            self.optimizer, milestones=[50, 75], gamma=0.1\n",
    "        )\n",
    "        self.epochs = epochs\n",
    "        self.best_acc = 0.0\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "        for epoch in range(self.epochs):\n",
    "            start_time = time.time()\n",
    "            running_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            for inputs, labels in self.train_loader:\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            self.scheduler.step()\n",
    "            epoch_acc = 100. * correct / total\n",
    "            epoch_time = time.time() - start_time\n",
    "            \n",
    "            print(f'Epoch [{epoch+1}/{self.epochs}] - Loss: {running_loss/len(self.train_loader):.4f} '\n",
    "                  f'Acc: {epoch_acc:.2f}% - Time: {epoch_time:.2f}s')\n",
    "        \n",
    "        print('Training finished!')\n",
    "        return self.model\n",
    "    \n",
    "    def save_model(self, model_path):\n",
    "        torch.save(self.model.state_dict(), model_path)\n",
    "        print(f'Model saved to {model_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa1c100",
   "metadata": {},
   "source": [
    "共形预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a23d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConformalPredictor:\n",
    "    def __init__(self, model, calib_loader, alpha=0.1, num_classes=10):\n",
    "        self.model = model\n",
    "        self.calib_loader = calib_loader\n",
    "        if self.model == 'resnet18':\n",
    "            self.alpha = 0.1\n",
    "        elif self.model == 'resnet34':\n",
    "            self.alpha = 0.05\n",
    "        else: # 'resnet50'\n",
    "            self.alpha = 0.03\n",
    "        self.num_classes = num_classes\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "        self.q = None\n",
    "    \n",
    "    def compute_scores(self, method='likelihood'):\n",
    "        print(\"Calibration method:\", method)\n",
    "        scores = []\n",
    "        model_to_use = self.model\n",
    "        model_to_use.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in self.calib_loader:\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                outputs = model_to_use(inputs)\n",
    "                \n",
    "                probs = torch.softmax(outputs, dim=1)\n",
    "                \n",
    "                batch_scores = None\n",
    "                if method == 'likelihood':\n",
    "                    batch_scores = 1 - probs[torch.arange(len(labels)), labels].cpu().numpy()\n",
    "                \n",
    "                elif method == 'cumulative':\n",
    "                    sorted_probs, _ = torch.sort(probs, dim=1, descending=True)\n",
    "                    cum_probs = torch.cumsum(sorted_probs, dim=1)\n",
    "                    ranks = (probs == probs.gather(1, labels.view(-1, 1))).nonzero()[:, 1]\n",
    "                    #match_matrix = (sorted_indices == labels.view(-1, 1))\n",
    "                    #ranks = match_matrix.int().argmax(dim=1)\n",
    "                    batch_scores = cum_probs[torch.arange(len(labels)), ranks].cpu().numpy()\n",
    "                \n",
    "                scores.extend(batch_scores)\n",
    "        \n",
    "        return np.array(scores)\n",
    "    \n",
    "    def calibrate(self, method='likelihood'):\n",
    "        if self.calib_loader is None:\n",
    "            raise ValueError(\"The calibration data loader is not set! Please provide a valid calib_loader.\")\n",
    "\n",
    "        scores = self.compute_scores(method)\n",
    "        \n",
    "        print(f\"Scores statistics ({method}):\")\n",
    "        print(f\"  Min: {np.min(scores):.6f}, Max: {np.max(scores):.6f}\")\n",
    "        print(f\"  Mean: {np.mean(scores):.6f}, Median: {np.median(scores):.6f}\")\n",
    "        print(f\"  10th percentile: {np.percentile(scores, 10):.6f}\")\n",
    "        print(f\"  90th percentile: {np.percentile(scores, 90):.6f}\")\n",
    "        \n",
    "        if method in ['likelihood', 'cumulative']:\n",
    "            self.q = np.percentile(scores, 100 * (1 - self.alpha))\n",
    "        \n",
    "        print(f'Calibration complete - Quantile: {self.q:.4f}')\n",
    "        return self.q\n",
    "    \n",
    "    def predict(self, inputs, method='likelihood'):\n",
    "        model_to_use = self.model\n",
    "        model_to_use.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            inputs = inputs.to(self.device)\n",
    "            outputs = model_to_use(inputs)\n",
    "            \n",
    "            probs = torch.softmax(outputs, dim=1)[0]\n",
    "\n",
    "            pred_set = None\n",
    "            if method == 'likelihood':\n",
    "                pred_mask = (1 - probs <= self.q)\n",
    "                pred_set = torch.nonzero(pred_mask).cpu().numpy().flatten()\n",
    "\n",
    "                if pred_set.size == 0:\n",
    "                    pred_set = np.array([probs.argmax().item()])\n",
    "        \n",
    "            elif method == 'cumulative':\n",
    "                sorted_probs, sorted_idxs = torch.sort(probs, descending=True)\n",
    "                cum_probs = torch.cumsum(sorted_probs, dim=0)\n",
    "        \n",
    "                mask = (1 - cum_probs <= self.q)\n",
    "                if mask.any():\n",
    "                    k = mask.nonzero()[0].item() + 1\n",
    "                else:\n",
    "                    k = 1\n",
    "\n",
    "                pred_set = sorted_idxs[:k].cpu().numpy()\n",
    "        \n",
    "            if pred_set.size == 0:\n",
    "                pred_set = np.array([probs.argmax().item()])\n",
    "        \n",
    "        return pred_set\n",
    "    \n",
    "    def evaluate(self, test_loader, method='likelihood', save_path=None):\n",
    "        coverage = []\n",
    "        set_sizes = []\n",
    "        prediction_sets = []\n",
    "        correct = []\n",
    "        losses = []\n",
    "        unconformity_scores = []\n",
    "        \n",
    "        model_to_use = self.model\n",
    "        model_to_use.eval()\n",
    "        criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "        \n",
    "        if save_path:\n",
    "            dir_path = os.path.dirname(save_path)\n",
    "            if dir_path:\n",
    "                os.makedirs(dir_path, exist_ok=True)\n",
    "            f = open(save_path, 'w')\n",
    "            f.write(\"sample_index,true_label,prediction_set,set_size,is_covered,is_correct,loss\\n\")\n",
    "        \n",
    "        sample_index = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                outputs = model_to_use(inputs)\n",
    "                \n",
    "                batch_losses = criterion(outputs, labels).cpu().numpy()\n",
    "                losses.extend(batch_losses)\n",
    "                \n",
    "                _, predicted = outputs.max(1)\n",
    "                batch_correct = predicted.eq(labels).cpu().numpy()\n",
    "                correct.extend(batch_correct)\n",
    "                \n",
    "                for i in range(inputs.size(0)):\n",
    "                    input_i = inputs[i].unsqueeze(0)\n",
    "                    output_i = model_to_use(input_i)\n",
    "                    prob_i = torch.softmax(output_i, dim=1)[0]\n",
    "                    \n",
    "                    true_label = labels[i].item()\n",
    "                    if method == 'likelihood':\n",
    "                        unconformity_score = 1 - prob_i[true_label].item()\n",
    "                    elif method == 'cumulative':\n",
    "                        sorted_probs, sorted_idxs = torch.sort(prob_i, descending=True)\n",
    "                        cum_probs = torch.cumsum(sorted_probs, dim=0)\n",
    "                        rank = (sorted_idxs == true_label).nonzero().item()\n",
    "                        unconformity_score = 1 - cum_probs[rank].item()\n",
    "                    \n",
    "                    unconformity_scores.append(unconformity_score)\n",
    "                    \n",
    "                    pred_set = self.predict(input_i, method)\n",
    "                    \n",
    "                    prediction_sets.append(pred_set.tolist())\n",
    "                    is_covered = true_label in pred_set\n",
    "                    coverage.append(is_covered)\n",
    "                    set_size = len(pred_set)\n",
    "                    set_sizes.append(set_size)\n",
    "                    \n",
    "                    is_correct = batch_correct[i]\n",
    "                    \n",
    "                    if save_path:\n",
    "                        set_str = \",\".join(map(str, pred_set))\n",
    "                        f.write(f\"{sample_index},{true_label},{set_str},{set_size},{int(is_covered)},{int(is_correct)},{batch_losses[i]:.6f}\\n\")\n",
    "                    \n",
    "                    sample_index += 1\n",
    "        \n",
    "        if save_path:\n",
    "            f.close()\n",
    "            print(f\"Prediction sets saved to: {save_path}\")\n",
    "        \n",
    "        coverage_rate = np.mean(coverage) * 100\n",
    "        avg_set_size = np.mean(set_sizes)\n",
    "        accuracy = np.mean(correct) * 100\n",
    "        avg_loss = np.mean(losses)\n",
    "        \n",
    "        print(f'Coverage: {coverage_rate:.2f}% | Avg Set Size: {avg_set_size:.2f} '\n",
    "              f'| Accuracy: {accuracy:.2f}% | Avg Loss: {avg_loss:.4f}')\n",
    "        \n",
    "        results = {\n",
    "            'coverage': coverage,\n",
    "            'set_size': set_sizes,\n",
    "            'prediction_sets': prediction_sets,\n",
    "            'correct': correct,\n",
    "            'loss': losses,\n",
    "            'unconformity_scores': unconformity_scores\n",
    "        }\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5f4ba3",
   "metadata": {},
   "source": [
    "train and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b4a1f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Dataset: cifar10\n",
      "\n",
      "Dataset sizes for CIFAR10:\n",
      "  Training set: 49000 samples\n",
      "  Calibration set: 1000 samples\n",
      "  Test set: 10000 samples\n",
      "\n",
      "===== Processing resnet18 =====\n",
      "Training model...\n",
      "Epoch [1/100] - Loss: 1.5879 Acc: 41.76% - Time: 11.67s\n",
      "Epoch [2/100] - Loss: 1.0798 Acc: 61.29% - Time: 11.03s\n",
      "Epoch [3/100] - Loss: 0.8431 Acc: 70.08% - Time: 11.14s\n",
      "Epoch [4/100] - Loss: 0.6974 Acc: 75.86% - Time: 11.19s\n",
      "Epoch [5/100] - Loss: 0.5988 Acc: 79.22% - Time: 11.25s\n",
      "Epoch [6/100] - Loss: 0.5309 Acc: 81.61% - Time: 11.21s\n",
      "Epoch [7/100] - Loss: 0.4717 Acc: 83.66% - Time: 11.27s\n",
      "Epoch [8/100] - Loss: 0.4343 Acc: 84.82% - Time: 11.32s\n",
      "Epoch [9/100] - Loss: 0.3999 Acc: 86.15% - Time: 11.36s\n",
      "Epoch [10/100] - Loss: 0.3765 Acc: 86.96% - Time: 11.35s\n",
      "Epoch [11/100] - Loss: 0.3480 Acc: 88.01% - Time: 11.30s\n",
      "Epoch [12/100] - Loss: 0.3279 Acc: 88.69% - Time: 11.47s\n",
      "Epoch [13/100] - Loss: 0.3089 Acc: 89.31% - Time: 11.42s\n",
      "Epoch [14/100] - Loss: 0.2878 Acc: 90.07% - Time: 11.43s\n",
      "Epoch [15/100] - Loss: 0.2761 Acc: 90.38% - Time: 11.49s\n",
      "Epoch [16/100] - Loss: 0.2701 Acc: 90.57% - Time: 11.49s\n",
      "Epoch [17/100] - Loss: 0.2522 Acc: 91.11% - Time: 11.46s\n",
      "Epoch [18/100] - Loss: 0.2442 Acc: 91.58% - Time: 11.44s\n",
      "Epoch [19/100] - Loss: 0.2351 Acc: 91.72% - Time: 11.48s\n",
      "Epoch [20/100] - Loss: 0.2298 Acc: 92.02% - Time: 11.36s\n",
      "Epoch [21/100] - Loss: 0.2184 Acc: 92.48% - Time: 11.48s\n",
      "Epoch [22/100] - Loss: 0.2167 Acc: 92.35% - Time: 11.45s\n",
      "Epoch [23/100] - Loss: 0.2089 Acc: 92.75% - Time: 11.48s\n",
      "Epoch [24/100] - Loss: 0.2043 Acc: 92.68% - Time: 11.47s\n",
      "Epoch [25/100] - Loss: 0.2047 Acc: 92.92% - Time: 11.48s\n",
      "Epoch [26/100] - Loss: 0.1981 Acc: 93.07% - Time: 11.43s\n",
      "Epoch [27/100] - Loss: 0.1885 Acc: 93.48% - Time: 11.47s\n",
      "Epoch [28/100] - Loss: 0.1867 Acc: 93.53% - Time: 11.41s\n",
      "Epoch [29/100] - Loss: 0.1883 Acc: 93.38% - Time: 11.43s\n",
      "Epoch [30/100] - Loss: 0.1826 Acc: 93.55% - Time: 11.46s\n",
      "Epoch [31/100] - Loss: 0.1751 Acc: 93.89% - Time: 11.47s\n",
      "Epoch [32/100] - Loss: 0.1801 Acc: 93.66% - Time: 11.49s\n",
      "Epoch [33/100] - Loss: 0.1763 Acc: 93.88% - Time: 11.49s\n",
      "Epoch [34/100] - Loss: 0.1725 Acc: 94.07% - Time: 11.47s\n",
      "Epoch [35/100] - Loss: 0.1680 Acc: 94.18% - Time: 11.47s\n",
      "Epoch [36/100] - Loss: 0.1717 Acc: 94.01% - Time: 11.48s\n",
      "Epoch [37/100] - Loss: 0.1676 Acc: 94.26% - Time: 11.44s\n",
      "Epoch [38/100] - Loss: 0.1642 Acc: 94.33% - Time: 11.52s\n",
      "Epoch [39/100] - Loss: 0.1625 Acc: 94.37% - Time: 11.46s\n",
      "Epoch [40/100] - Loss: 0.1607 Acc: 94.47% - Time: 11.48s\n",
      "Epoch [41/100] - Loss: 0.1623 Acc: 94.48% - Time: 11.55s\n",
      "Epoch [42/100] - Loss: 0.1610 Acc: 94.49% - Time: 11.46s\n",
      "Epoch [43/100] - Loss: 0.1549 Acc: 94.65% - Time: 11.48s\n",
      "Epoch [44/100] - Loss: 0.1591 Acc: 94.55% - Time: 11.52s\n",
      "Epoch [45/100] - Loss: 0.1491 Acc: 94.85% - Time: 11.49s\n",
      "Epoch [46/100] - Loss: 0.1503 Acc: 94.89% - Time: 11.39s\n",
      "Epoch [47/100] - Loss: 0.1573 Acc: 94.63% - Time: 11.47s\n",
      "Epoch [48/100] - Loss: 0.1467 Acc: 95.01% - Time: 11.51s\n",
      "Epoch [49/100] - Loss: 0.1533 Acc: 94.83% - Time: 11.50s\n",
      "Epoch [50/100] - Loss: 0.1468 Acc: 95.05% - Time: 11.50s\n",
      "Epoch [51/100] - Loss: 0.0673 Acc: 97.87% - Time: 11.49s\n",
      "Epoch [52/100] - Loss: 0.0411 Acc: 98.83% - Time: 11.51s\n",
      "Epoch [53/100] - Loss: 0.0320 Acc: 99.12% - Time: 11.52s\n",
      "Epoch [54/100] - Loss: 0.0274 Acc: 99.25% - Time: 11.35s\n",
      "Epoch [55/100] - Loss: 0.0220 Acc: 99.44% - Time: 11.42s\n",
      "Epoch [56/100] - Loss: 0.0201 Acc: 99.46% - Time: 11.46s\n",
      "Epoch [57/100] - Loss: 0.0177 Acc: 99.56% - Time: 11.49s\n",
      "Epoch [58/100] - Loss: 0.0165 Acc: 99.57% - Time: 11.45s\n",
      "Epoch [59/100] - Loss: 0.0142 Acc: 99.66% - Time: 11.41s\n",
      "Epoch [60/100] - Loss: 0.0127 Acc: 99.71% - Time: 11.49s\n",
      "Epoch [61/100] - Loss: 0.0120 Acc: 99.73% - Time: 11.46s\n",
      "Epoch [62/100] - Loss: 0.0101 Acc: 99.78% - Time: 11.44s\n",
      "Epoch [63/100] - Loss: 0.0099 Acc: 99.79% - Time: 11.30s\n",
      "Epoch [64/100] - Loss: 0.0089 Acc: 99.84% - Time: 11.47s\n",
      "Epoch [65/100] - Loss: 0.0085 Acc: 99.83% - Time: 11.51s\n",
      "Epoch [66/100] - Loss: 0.0088 Acc: 99.83% - Time: 11.48s\n",
      "Epoch [67/100] - Loss: 0.0078 Acc: 99.85% - Time: 11.43s\n",
      "Epoch [68/100] - Loss: 0.0072 Acc: 99.87% - Time: 11.45s\n",
      "Epoch [69/100] - Loss: 0.0068 Acc: 99.89% - Time: 11.50s\n",
      "Epoch [70/100] - Loss: 0.0065 Acc: 99.87% - Time: 11.46s\n",
      "Epoch [71/100] - Loss: 0.0063 Acc: 99.90% - Time: 11.42s\n",
      "Epoch [72/100] - Loss: 0.0060 Acc: 99.89% - Time: 11.40s\n",
      "Epoch [73/100] - Loss: 0.0054 Acc: 99.92% - Time: 11.48s\n",
      "Epoch [74/100] - Loss: 0.0065 Acc: 99.87% - Time: 11.50s\n",
      "Epoch [75/100] - Loss: 0.0059 Acc: 99.89% - Time: 11.45s\n",
      "Epoch [76/100] - Loss: 0.0056 Acc: 99.91% - Time: 11.48s\n",
      "Epoch [77/100] - Loss: 0.0050 Acc: 99.93% - Time: 11.49s\n",
      "Epoch [78/100] - Loss: 0.0045 Acc: 99.92% - Time: 11.47s\n",
      "Epoch [79/100] - Loss: 0.0045 Acc: 99.93% - Time: 11.46s\n",
      "Epoch [80/100] - Loss: 0.0046 Acc: 99.93% - Time: 11.33s\n",
      "Epoch [81/100] - Loss: 0.0045 Acc: 99.94% - Time: 11.48s\n",
      "Epoch [82/100] - Loss: 0.0044 Acc: 99.95% - Time: 11.48s\n",
      "Epoch [83/100] - Loss: 0.0043 Acc: 99.95% - Time: 11.47s\n",
      "Epoch [84/100] - Loss: 0.0042 Acc: 99.94% - Time: 11.44s\n",
      "Epoch [85/100] - Loss: 0.0044 Acc: 99.93% - Time: 11.48s\n",
      "Epoch [86/100] - Loss: 0.0039 Acc: 99.96% - Time: 11.51s\n",
      "Epoch [87/100] - Loss: 0.0044 Acc: 99.95% - Time: 11.46s\n",
      "Epoch [88/100] - Loss: 0.0040 Acc: 99.96% - Time: 11.46s\n",
      "Epoch [89/100] - Loss: 0.0039 Acc: 99.95% - Time: 11.38s\n",
      "Epoch [90/100] - Loss: 0.0040 Acc: 99.95% - Time: 11.49s\n",
      "Epoch [91/100] - Loss: 0.0039 Acc: 99.96% - Time: 11.54s\n",
      "Epoch [92/100] - Loss: 0.0038 Acc: 99.96% - Time: 11.48s\n",
      "Epoch [93/100] - Loss: 0.0035 Acc: 99.98% - Time: 11.49s\n",
      "Epoch [94/100] - Loss: 0.0038 Acc: 99.97% - Time: 11.48s\n",
      "Epoch [95/100] - Loss: 0.0042 Acc: 99.93% - Time: 11.49s\n",
      "Epoch [96/100] - Loss: 0.0038 Acc: 99.96% - Time: 11.47s\n",
      "Epoch [97/100] - Loss: 0.0038 Acc: 99.96% - Time: 11.42s\n",
      "Epoch [98/100] - Loss: 0.0037 Acc: 99.96% - Time: 11.44s\n",
      "Epoch [99/100] - Loss: 0.0037 Acc: 99.96% - Time: 11.47s\n",
      "Epoch [100/100] - Loss: 0.0036 Acc: 99.97% - Time: 11.41s\n",
      "Training finished!\n",
      "Model saved to resnet18_cifar10_0.pth\n",
      "\n",
      "===== Processing resnet34 =====\n",
      "Training model...\n",
      "Epoch [1/100] - Loss: 1.7178 Acc: 37.40% - Time: 20.93s\n",
      "Epoch [2/100] - Loss: 1.1767 Acc: 57.54% - Time: 20.99s\n",
      "Epoch [3/100] - Loss: 0.9061 Acc: 67.90% - Time: 20.98s\n",
      "Epoch [4/100] - Loss: 0.7223 Acc: 74.60% - Time: 20.96s\n",
      "Epoch [5/100] - Loss: 0.6184 Acc: 78.58% - Time: 21.03s\n",
      "Epoch [6/100] - Loss: 0.5493 Acc: 80.93% - Time: 21.04s\n",
      "Epoch [7/100] - Loss: 0.4877 Acc: 83.39% - Time: 20.99s\n",
      "Epoch [8/100] - Loss: 0.4511 Acc: 84.41% - Time: 20.98s\n",
      "Epoch [9/100] - Loss: 0.4193 Acc: 85.53% - Time: 21.04s\n",
      "Epoch [10/100] - Loss: 0.3916 Acc: 86.48% - Time: 21.00s\n",
      "Epoch [11/100] - Loss: 0.3650 Acc: 87.38% - Time: 21.05s\n",
      "Epoch [12/100] - Loss: 0.3491 Acc: 87.98% - Time: 21.04s\n",
      "Epoch [13/100] - Loss: 0.3253 Acc: 88.79% - Time: 20.97s\n",
      "Epoch [14/100] - Loss: 0.3121 Acc: 89.14% - Time: 20.79s\n",
      "Epoch [15/100] - Loss: 0.2983 Acc: 89.63% - Time: 20.81s\n",
      "Epoch [16/100] - Loss: 0.2888 Acc: 89.95% - Time: 20.82s\n",
      "Epoch [17/100] - Loss: 0.2722 Acc: 90.62% - Time: 21.05s\n",
      "Epoch [18/100] - Loss: 0.2590 Acc: 91.11% - Time: 21.01s\n",
      "Epoch [19/100] - Loss: 0.2482 Acc: 91.38% - Time: 20.99s\n",
      "Epoch [20/100] - Loss: 0.2424 Acc: 91.69% - Time: 20.97s\n",
      "Epoch [21/100] - Loss: 0.2337 Acc: 91.99% - Time: 21.05s\n",
      "Epoch [22/100] - Loss: 0.2270 Acc: 92.18% - Time: 21.03s\n",
      "Epoch [23/100] - Loss: 0.2144 Acc: 92.55% - Time: 21.05s\n",
      "Epoch [24/100] - Loss: 0.2150 Acc: 92.48% - Time: 21.05s\n",
      "Epoch [25/100] - Loss: 0.2071 Acc: 92.89% - Time: 20.92s\n",
      "Epoch [26/100] - Loss: 0.2013 Acc: 93.00% - Time: 21.03s\n",
      "Epoch [27/100] - Loss: 0.1958 Acc: 93.27% - Time: 21.05s\n",
      "Epoch [28/100] - Loss: 0.1967 Acc: 93.19% - Time: 21.08s\n",
      "Epoch [29/100] - Loss: 0.1854 Acc: 93.59% - Time: 20.94s\n",
      "Epoch [30/100] - Loss: 0.1842 Acc: 93.65% - Time: 20.99s\n",
      "Epoch [31/100] - Loss: 0.1848 Acc: 93.51% - Time: 20.96s\n",
      "Epoch [32/100] - Loss: 0.1819 Acc: 93.70% - Time: 20.99s\n",
      "Epoch [33/100] - Loss: 0.1783 Acc: 93.79% - Time: 20.99s\n",
      "Epoch [34/100] - Loss: 0.1740 Acc: 93.97% - Time: 20.89s\n",
      "Epoch [35/100] - Loss: 0.1687 Acc: 94.21% - Time: 21.03s\n",
      "Epoch [36/100] - Loss: 0.1707 Acc: 94.14% - Time: 21.03s\n",
      "Epoch [37/100] - Loss: 0.1684 Acc: 94.08% - Time: 21.01s\n",
      "Epoch [38/100] - Loss: 0.1671 Acc: 94.29% - Time: 21.01s\n",
      "Epoch [39/100] - Loss: 0.1663 Acc: 94.27% - Time: 20.99s\n",
      "Epoch [40/100] - Loss: 0.1573 Acc: 94.50% - Time: 20.98s\n",
      "Epoch [41/100] - Loss: 0.1552 Acc: 94.62% - Time: 21.05s\n",
      "Epoch [42/100] - Loss: 0.1597 Acc: 94.52% - Time: 20.97s\n",
      "Epoch [43/100] - Loss: 0.1547 Acc: 94.67% - Time: 21.04s\n",
      "Epoch [44/100] - Loss: 0.1570 Acc: 94.62% - Time: 21.01s\n",
      "Epoch [45/100] - Loss: 0.1516 Acc: 94.74% - Time: 21.13s\n",
      "Epoch [46/100] - Loss: 0.1512 Acc: 94.74% - Time: 21.22s\n",
      "Epoch [47/100] - Loss: 0.1499 Acc: 94.94% - Time: 21.17s\n",
      "Epoch [48/100] - Loss: 0.1490 Acc: 94.87% - Time: 21.17s\n",
      "Epoch [49/100] - Loss: 0.1445 Acc: 95.04% - Time: 21.25s\n",
      "Epoch [50/100] - Loss: 0.1476 Acc: 94.94% - Time: 21.24s\n",
      "Epoch [51/100] - Loss: 0.0597 Acc: 98.08% - Time: 21.22s\n",
      "Epoch [52/100] - Loss: 0.0365 Acc: 98.96% - Time: 21.17s\n",
      "Epoch [53/100] - Loss: 0.0277 Acc: 99.18% - Time: 21.23s\n",
      "Epoch [54/100] - Loss: 0.0223 Acc: 99.36% - Time: 21.25s\n",
      "Epoch [55/100] - Loss: 0.0191 Acc: 99.43% - Time: 21.26s\n",
      "Epoch [56/100] - Loss: 0.0166 Acc: 99.52% - Time: 21.27s\n",
      "Epoch [57/100] - Loss: 0.0140 Acc: 99.62% - Time: 21.28s\n",
      "Epoch [58/100] - Loss: 0.0124 Acc: 99.68% - Time: 21.27s\n",
      "Epoch [59/100] - Loss: 0.0114 Acc: 99.71% - Time: 21.26s\n",
      "Epoch [60/100] - Loss: 0.0099 Acc: 99.75% - Time: 21.12s\n",
      "Epoch [61/100] - Loss: 0.0090 Acc: 99.77% - Time: 21.22s\n",
      "Epoch [62/100] - Loss: 0.0079 Acc: 99.81% - Time: 21.24s\n",
      "Epoch [63/100] - Loss: 0.0076 Acc: 99.81% - Time: 21.29s\n",
      "Epoch [64/100] - Loss: 0.0072 Acc: 99.83% - Time: 21.31s\n",
      "Epoch [65/100] - Loss: 0.0066 Acc: 99.84% - Time: 21.33s\n",
      "Epoch [66/100] - Loss: 0.0064 Acc: 99.85% - Time: 21.28s\n",
      "Epoch [67/100] - Loss: 0.0052 Acc: 99.89% - Time: 21.27s\n",
      "Epoch [68/100] - Loss: 0.0051 Acc: 99.91% - Time: 21.25s\n",
      "Epoch [69/100] - Loss: 0.0051 Acc: 99.90% - Time: 21.29s\n",
      "Epoch [70/100] - Loss: 0.0044 Acc: 99.91% - Time: 21.26s\n",
      "Epoch [71/100] - Loss: 0.0046 Acc: 99.90% - Time: 21.27s\n",
      "Epoch [72/100] - Loss: 0.0043 Acc: 99.93% - Time: 21.22s\n",
      "Epoch [73/100] - Loss: 0.0049 Acc: 99.87% - Time: 21.22s\n",
      "Epoch [74/100] - Loss: 0.0046 Acc: 99.88% - Time: 21.28s\n",
      "Epoch [75/100] - Loss: 0.0038 Acc: 99.93% - Time: 21.23s\n",
      "Epoch [76/100] - Loss: 0.0032 Acc: 99.95% - Time: 21.23s\n",
      "Epoch [77/100] - Loss: 0.0035 Acc: 99.93% - Time: 21.35s\n",
      "Epoch [78/100] - Loss: 0.0028 Acc: 99.96% - Time: 21.30s\n",
      "Epoch [79/100] - Loss: 0.0031 Acc: 99.94% - Time: 21.24s\n",
      "Epoch [80/100] - Loss: 0.0029 Acc: 99.95% - Time: 21.18s\n",
      "Epoch [81/100] - Loss: 0.0028 Acc: 99.96% - Time: 21.22s\n",
      "Epoch [82/100] - Loss: 0.0028 Acc: 99.97% - Time: 21.25s\n",
      "Epoch [83/100] - Loss: 0.0030 Acc: 99.94% - Time: 21.17s\n",
      "Epoch [84/100] - Loss: 0.0031 Acc: 99.93% - Time: 21.20s\n",
      "Epoch [85/100] - Loss: 0.0030 Acc: 99.94% - Time: 21.26s\n",
      "Epoch [86/100] - Loss: 0.0024 Acc: 99.97% - Time: 21.22s\n",
      "Epoch [87/100] - Loss: 0.0025 Acc: 99.96% - Time: 21.22s\n",
      "Epoch [88/100] - Loss: 0.0025 Acc: 99.97% - Time: 21.20s\n",
      "Epoch [89/100] - Loss: 0.0024 Acc: 99.98% - Time: 21.22s\n",
      "Epoch [90/100] - Loss: 0.0023 Acc: 99.97% - Time: 21.26s\n",
      "Epoch [91/100] - Loss: 0.0025 Acc: 99.97% - Time: 21.27s\n",
      "Epoch [92/100] - Loss: 0.0024 Acc: 99.95% - Time: 21.20s\n",
      "Epoch [93/100] - Loss: 0.0023 Acc: 99.96% - Time: 21.24s\n",
      "Epoch [94/100] - Loss: 0.0025 Acc: 99.96% - Time: 21.22s\n",
      "Epoch [95/100] - Loss: 0.0023 Acc: 99.97% - Time: 21.24s\n",
      "Epoch [96/100] - Loss: 0.0025 Acc: 99.96% - Time: 21.24s\n",
      "Epoch [97/100] - Loss: 0.0023 Acc: 99.97% - Time: 21.31s\n",
      "Epoch [98/100] - Loss: 0.0024 Acc: 99.97% - Time: 21.26s\n",
      "Epoch [99/100] - Loss: 0.0022 Acc: 99.97% - Time: 21.34s\n",
      "Epoch [100/100] - Loss: 0.0021 Acc: 99.97% - Time: 21.26s\n",
      "Training finished!\n",
      "Model saved to resnet34_cifar10_0.pth\n",
      "\n",
      "===== Processing resnet50 =====\n",
      "Training model...\n",
      "Epoch [1/100] - Loss: 1.7627 Acc: 35.81% - Time: 21.30s\n",
      "Epoch [2/100] - Loss: 1.2457 Acc: 54.78% - Time: 21.23s\n",
      "Epoch [3/100] - Loss: 0.9515 Acc: 66.23% - Time: 21.28s\n",
      "Epoch [4/100] - Loss: 0.7582 Acc: 73.47% - Time: 21.25s\n",
      "Epoch [5/100] - Loss: 0.6460 Acc: 77.37% - Time: 21.27s\n",
      "Epoch [6/100] - Loss: 0.5614 Acc: 80.65% - Time: 21.21s\n",
      "Epoch [7/100] - Loss: 0.5055 Acc: 82.48% - Time: 21.15s\n",
      "Epoch [8/100] - Loss: 0.4617 Acc: 84.06% - Time: 21.19s\n",
      "Epoch [9/100] - Loss: 0.4235 Acc: 85.46% - Time: 21.14s\n",
      "Epoch [10/100] - Loss: 0.3909 Acc: 86.58% - Time: 21.21s\n",
      "Epoch [11/100] - Loss: 0.3667 Acc: 87.42% - Time: 21.17s\n",
      "Epoch [12/100] - Loss: 0.3460 Acc: 88.06% - Time: 21.14s\n",
      "Epoch [13/100] - Loss: 0.3263 Acc: 88.73% - Time: 21.18s\n",
      "Epoch [14/100] - Loss: 0.3068 Acc: 89.43% - Time: 21.18s\n",
      "Epoch [15/100] - Loss: 0.2955 Acc: 89.85% - Time: 21.16s\n",
      "Epoch [16/100] - Loss: 0.2807 Acc: 90.33% - Time: 21.09s\n",
      "Epoch [17/100] - Loss: 0.2700 Acc: 90.77% - Time: 21.17s\n",
      "Epoch [18/100] - Loss: 0.2584 Acc: 91.09% - Time: 21.17s\n",
      "Epoch [19/100] - Loss: 0.2481 Acc: 91.54% - Time: 21.20s\n",
      "Epoch [20/100] - Loss: 0.2418 Acc: 91.68% - Time: 21.17s\n",
      "Epoch [21/100] - Loss: 0.2312 Acc: 92.03% - Time: 21.17s\n",
      "Epoch [22/100] - Loss: 0.2271 Acc: 92.07% - Time: 21.24s\n",
      "Epoch [23/100] - Loss: 0.2215 Acc: 92.33% - Time: 21.19s\n",
      "Epoch [24/100] - Loss: 0.2104 Acc: 92.76% - Time: 21.16s\n",
      "Epoch [25/100] - Loss: 0.2097 Acc: 92.80% - Time: 20.79s\n",
      "Epoch [26/100] - Loss: 0.2017 Acc: 92.97% - Time: 20.76s\n",
      "Epoch [27/100] - Loss: 0.1960 Acc: 93.26% - Time: 20.68s\n",
      "Epoch [28/100] - Loss: 0.1974 Acc: 93.19% - Time: 20.70s\n",
      "Epoch [29/100] - Loss: 0.1893 Acc: 93.51% - Time: 20.70s\n",
      "Epoch [30/100] - Loss: 0.1885 Acc: 93.54% - Time: 20.80s\n",
      "Epoch [31/100] - Loss: 0.1858 Acc: 93.59% - Time: 20.80s\n",
      "Epoch [32/100] - Loss: 0.1855 Acc: 93.64% - Time: 20.75s\n",
      "Epoch [33/100] - Loss: 0.1800 Acc: 93.77% - Time: 20.71s\n",
      "Epoch [34/100] - Loss: 0.1743 Acc: 93.99% - Time: 20.72s\n",
      "Epoch [35/100] - Loss: 0.1748 Acc: 93.98% - Time: 20.79s\n",
      "Epoch [36/100] - Loss: 0.1722 Acc: 93.97% - Time: 20.65s\n",
      "Epoch [37/100] - Loss: 0.1673 Acc: 94.28% - Time: 20.81s\n",
      "Epoch [38/100] - Loss: 0.1668 Acc: 94.18% - Time: 20.71s\n",
      "Epoch [39/100] - Loss: 0.1599 Acc: 94.46% - Time: 20.71s\n",
      "Epoch [40/100] - Loss: 0.1600 Acc: 94.38% - Time: 20.67s\n",
      "Epoch [41/100] - Loss: 0.1565 Acc: 94.64% - Time: 20.63s\n",
      "Epoch [42/100] - Loss: 0.1641 Acc: 94.29% - Time: 20.67s\n",
      "Epoch [43/100] - Loss: 0.1585 Acc: 94.46% - Time: 20.68s\n",
      "Epoch [44/100] - Loss: 0.1545 Acc: 94.70% - Time: 20.70s\n",
      "Epoch [45/100] - Loss: 0.1567 Acc: 94.72% - Time: 20.66s\n",
      "Epoch [46/100] - Loss: 0.1526 Acc: 94.74% - Time: 20.70s\n",
      "Epoch [47/100] - Loss: 0.1501 Acc: 94.79% - Time: 20.65s\n",
      "Epoch [48/100] - Loss: 0.1497 Acc: 94.80% - Time: 20.63s\n",
      "Epoch [49/100] - Loss: 0.1450 Acc: 95.07% - Time: 20.64s\n",
      "Epoch [50/100] - Loss: 0.1453 Acc: 94.93% - Time: 20.80s\n",
      "Epoch [51/100] - Loss: 0.0655 Acc: 97.97% - Time: 20.63s\n",
      "Epoch [52/100] - Loss: 0.0370 Acc: 98.91% - Time: 20.65s\n",
      "Epoch [53/100] - Loss: 0.0283 Acc: 99.19% - Time: 20.70s\n",
      "Epoch [54/100] - Loss: 0.0241 Acc: 99.29% - Time: 20.82s\n",
      "Epoch [55/100] - Loss: 0.0189 Acc: 99.47% - Time: 20.68s\n",
      "Epoch [56/100] - Loss: 0.0151 Acc: 99.59% - Time: 20.62s\n",
      "Epoch [57/100] - Loss: 0.0144 Acc: 99.62% - Time: 20.68s\n",
      "Epoch [58/100] - Loss: 0.0122 Acc: 99.70% - Time: 20.64s\n",
      "Epoch [59/100] - Loss: 0.0116 Acc: 99.69% - Time: 20.67s\n",
      "Epoch [60/100] - Loss: 0.0100 Acc: 99.74% - Time: 20.77s\n",
      "Epoch [61/100] - Loss: 0.0087 Acc: 99.78% - Time: 20.71s\n",
      "Epoch [62/100] - Loss: 0.0080 Acc: 99.79% - Time: 20.71s\n",
      "Epoch [63/100] - Loss: 0.0079 Acc: 99.81% - Time: 20.63s\n",
      "Epoch [64/100] - Loss: 0.0072 Acc: 99.80% - Time: 20.73s\n",
      "Epoch [65/100] - Loss: 0.0066 Acc: 99.81% - Time: 20.63s\n",
      "Epoch [66/100] - Loss: 0.0064 Acc: 99.84% - Time: 20.74s\n",
      "Epoch [67/100] - Loss: 0.0056 Acc: 99.87% - Time: 20.60s\n",
      "Epoch [68/100] - Loss: 0.0055 Acc: 99.86% - Time: 20.72s\n",
      "Epoch [69/100] - Loss: 0.0059 Acc: 99.85% - Time: 20.67s\n",
      "Epoch [70/100] - Loss: 0.0061 Acc: 99.85% - Time: 20.70s\n",
      "Epoch [71/100] - Loss: 0.0052 Acc: 99.88% - Time: 20.83s\n",
      "Epoch [72/100] - Loss: 0.0048 Acc: 99.89% - Time: 20.65s\n",
      "Epoch [73/100] - Loss: 0.0046 Acc: 99.90% - Time: 20.68s\n",
      "Epoch [74/100] - Loss: 0.0044 Acc: 99.90% - Time: 20.67s\n",
      "Epoch [75/100] - Loss: 0.0042 Acc: 99.91% - Time: 20.76s\n",
      "Epoch [76/100] - Loss: 0.0033 Acc: 99.95% - Time: 20.71s\n",
      "Epoch [77/100] - Loss: 0.0033 Acc: 99.93% - Time: 20.72s\n",
      "Epoch [78/100] - Loss: 0.0032 Acc: 99.93% - Time: 20.72s\n",
      "Epoch [79/100] - Loss: 0.0029 Acc: 99.95% - Time: 20.71s\n",
      "Epoch [80/100] - Loss: 0.0028 Acc: 99.96% - Time: 20.71s\n",
      "Epoch [81/100] - Loss: 0.0028 Acc: 99.95% - Time: 20.67s\n",
      "Epoch [82/100] - Loss: 0.0029 Acc: 99.94% - Time: 20.67s\n",
      "Epoch [83/100] - Loss: 0.0029 Acc: 99.95% - Time: 20.63s\n",
      "Epoch [84/100] - Loss: 0.0030 Acc: 99.94% - Time: 20.68s\n",
      "Epoch [85/100] - Loss: 0.0027 Acc: 99.96% - Time: 20.64s\n",
      "Epoch [86/100] - Loss: 0.0027 Acc: 99.96% - Time: 20.69s\n",
      "Epoch [87/100] - Loss: 0.0025 Acc: 99.96% - Time: 20.64s\n",
      "Epoch [88/100] - Loss: 0.0025 Acc: 99.97% - Time: 20.68s\n",
      "Epoch [89/100] - Loss: 0.0026 Acc: 99.97% - Time: 20.85s\n",
      "Epoch [90/100] - Loss: 0.0026 Acc: 99.96% - Time: 20.74s\n",
      "Epoch [91/100] - Loss: 0.0023 Acc: 99.97% - Time: 20.66s\n",
      "Epoch [92/100] - Loss: 0.0026 Acc: 99.97% - Time: 20.63s\n",
      "Epoch [93/100] - Loss: 0.0021 Acc: 99.98% - Time: 20.70s\n",
      "Epoch [94/100] - Loss: 0.0024 Acc: 99.96% - Time: 20.69s\n",
      "Epoch [95/100] - Loss: 0.0025 Acc: 99.96% - Time: 20.78s\n",
      "Epoch [96/100] - Loss: 0.0026 Acc: 99.96% - Time: 20.60s\n",
      "Epoch [97/100] - Loss: 0.0027 Acc: 99.95% - Time: 20.66s\n",
      "Epoch [98/100] - Loss: 0.0024 Acc: 99.96% - Time: 20.65s\n",
      "Epoch [99/100] - Loss: 0.0023 Acc: 99.96% - Time: 20.70s\n",
      "Epoch [100/100] - Loss: 0.0022 Acc: 99.96% - Time: 20.68s\n",
      "Training finished!\n",
      "Model saved to resnet50_cifar10_0.pth\n",
      "Loading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: cifar100\n",
      "\n",
      "Dataset sizes for CIFAR100:\n",
      "  Training set: 49000 samples\n",
      "  Calibration set: 1000 samples\n",
      "  Test set: 10000 samples\n",
      "\n",
      "===== Processing resnet18 =====\n",
      "Training model...\n",
      "Epoch [1/100] - Loss: 3.8369 Acc: 11.21% - Time: 11.18s\n",
      "Epoch [2/100] - Loss: 3.1472 Acc: 22.16% - Time: 11.16s\n",
      "Epoch [3/100] - Loss: 2.6416 Acc: 31.58% - Time: 11.14s\n",
      "Epoch [4/100] - Loss: 2.2286 Acc: 40.23% - Time: 11.27s\n",
      "Epoch [5/100] - Loss: 1.9413 Acc: 46.95% - Time: 11.29s\n",
      "Epoch [6/100] - Loss: 1.7314 Acc: 51.43% - Time: 11.26s\n",
      "Epoch [7/100] - Loss: 1.5736 Acc: 55.71% - Time: 11.30s\n",
      "Epoch [8/100] - Loss: 1.4501 Acc: 58.63% - Time: 11.40s\n",
      "Epoch [9/100] - Loss: 1.3354 Acc: 61.35% - Time: 11.37s\n",
      "Epoch [10/100] - Loss: 1.2450 Acc: 63.67% - Time: 11.28s\n",
      "Epoch [11/100] - Loss: 1.1729 Acc: 65.66% - Time: 11.24s\n",
      "Epoch [12/100] - Loss: 1.0962 Acc: 67.74% - Time: 11.23s\n",
      "Epoch [13/100] - Loss: 1.0375 Acc: 69.19% - Time: 11.24s\n",
      "Epoch [14/100] - Loss: 0.9753 Acc: 70.90% - Time: 11.24s\n",
      "Epoch [15/100] - Loss: 0.9253 Acc: 72.14% - Time: 11.24s\n",
      "Epoch [16/100] - Loss: 0.8808 Acc: 73.54% - Time: 11.25s\n",
      "Epoch [17/100] - Loss: 0.8380 Acc: 74.84% - Time: 11.23s\n",
      "Epoch [18/100] - Loss: 0.7931 Acc: 75.82% - Time: 11.24s\n",
      "Epoch [19/100] - Loss: 0.7604 Acc: 76.83% - Time: 11.27s\n",
      "Epoch [20/100] - Loss: 0.7373 Acc: 77.46% - Time: 11.27s\n",
      "Epoch [21/100] - Loss: 0.7057 Acc: 78.35% - Time: 11.32s\n",
      "Epoch [22/100] - Loss: 0.6804 Acc: 79.22% - Time: 11.25s\n",
      "Epoch [23/100] - Loss: 0.6450 Acc: 80.18% - Time: 11.36s\n",
      "Epoch [24/100] - Loss: 0.6303 Acc: 80.51% - Time: 11.42s\n",
      "Epoch [25/100] - Loss: 0.6106 Acc: 81.01% - Time: 11.37s\n",
      "Epoch [26/100] - Loss: 0.5787 Acc: 82.12% - Time: 11.28s\n",
      "Epoch [27/100] - Loss: 0.5727 Acc: 82.28% - Time: 11.22s\n",
      "Epoch [28/100] - Loss: 0.5585 Acc: 82.68% - Time: 11.23s\n",
      "Epoch [29/100] - Loss: 0.5382 Acc: 83.42% - Time: 11.26s\n",
      "Epoch [30/100] - Loss: 0.5228 Acc: 83.87% - Time: 11.30s\n",
      "Epoch [31/100] - Loss: 0.5047 Acc: 84.28% - Time: 11.28s\n",
      "Epoch [32/100] - Loss: 0.5009 Acc: 84.32% - Time: 11.30s\n",
      "Epoch [33/100] - Loss: 0.4777 Acc: 85.21% - Time: 11.32s\n",
      "Epoch [34/100] - Loss: 0.4795 Acc: 84.99% - Time: 11.27s\n",
      "Epoch [35/100] - Loss: 0.4649 Acc: 85.55% - Time: 11.33s\n",
      "Epoch [36/100] - Loss: 0.4558 Acc: 85.72% - Time: 11.26s\n",
      "Epoch [37/100] - Loss: 0.4393 Acc: 86.36% - Time: 11.41s\n",
      "Epoch [38/100] - Loss: 0.4481 Acc: 86.04% - Time: 11.47s\n",
      "Epoch [39/100] - Loss: 0.4321 Acc: 86.56% - Time: 11.35s\n",
      "Epoch [40/100] - Loss: 0.4147 Acc: 87.08% - Time: 11.43s\n",
      "Epoch [41/100] - Loss: 0.4164 Acc: 87.22% - Time: 11.37s\n",
      "Epoch [42/100] - Loss: 0.4110 Acc: 87.15% - Time: 11.36s\n",
      "Epoch [43/100] - Loss: 0.4103 Acc: 87.14% - Time: 11.44s\n",
      "Epoch [44/100] - Loss: 0.3932 Acc: 87.99% - Time: 11.26s\n",
      "Epoch [45/100] - Loss: 0.3923 Acc: 87.86% - Time: 11.38s\n",
      "Epoch [46/100] - Loss: 0.3841 Acc: 88.03% - Time: 11.57s\n",
      "Epoch [47/100] - Loss: 0.3775 Acc: 88.57% - Time: 11.54s\n",
      "Epoch [48/100] - Loss: 0.3746 Acc: 88.30% - Time: 11.52s\n",
      "Epoch [49/100] - Loss: 0.3787 Acc: 88.21% - Time: 11.38s\n",
      "Epoch [50/100] - Loss: 0.3712 Acc: 88.56% - Time: 11.49s\n",
      "Epoch [51/100] - Loss: 0.1639 Acc: 95.53% - Time: 11.51s\n",
      "Epoch [52/100] - Loss: 0.0967 Acc: 97.92% - Time: 11.50s\n",
      "Epoch [53/100] - Loss: 0.0776 Acc: 98.51% - Time: 11.53s\n",
      "Epoch [54/100] - Loss: 0.0658 Acc: 98.91% - Time: 11.55s\n",
      "Epoch [55/100] - Loss: 0.0569 Acc: 99.10% - Time: 11.46s\n",
      "Epoch [56/100] - Loss: 0.0513 Acc: 99.26% - Time: 11.47s\n",
      "Epoch [57/100] - Loss: 0.0486 Acc: 99.33% - Time: 11.54s\n",
      "Epoch [58/100] - Loss: 0.0447 Acc: 99.40% - Time: 11.59s\n",
      "Epoch [59/100] - Loss: 0.0420 Acc: 99.47% - Time: 11.53s\n",
      "Epoch [60/100] - Loss: 0.0382 Acc: 99.54% - Time: 11.65s\n",
      "Epoch [61/100] - Loss: 0.0361 Acc: 99.61% - Time: 11.52s\n",
      "Epoch [62/100] - Loss: 0.0341 Acc: 99.66% - Time: 11.67s\n",
      "Epoch [63/100] - Loss: 0.0329 Acc: 99.68% - Time: 11.68s\n",
      "Epoch [64/100] - Loss: 0.0313 Acc: 99.72% - Time: 11.65s\n",
      "Epoch [65/100] - Loss: 0.0303 Acc: 99.73% - Time: 11.63s\n",
      "Epoch [66/100] - Loss: 0.0290 Acc: 99.77% - Time: 11.65s\n",
      "Epoch [67/100] - Loss: 0.0278 Acc: 99.78% - Time: 11.66s\n",
      "Epoch [68/100] - Loss: 0.0270 Acc: 99.78% - Time: 11.41s\n",
      "Epoch [69/100] - Loss: 0.0263 Acc: 99.81% - Time: 11.32s\n",
      "Epoch [70/100] - Loss: 0.0252 Acc: 99.84% - Time: 11.66s\n",
      "Epoch [71/100] - Loss: 0.0246 Acc: 99.84% - Time: 11.56s\n",
      "Epoch [72/100] - Loss: 0.0237 Acc: 99.86% - Time: 11.56s\n",
      "Epoch [73/100] - Loss: 0.0231 Acc: 99.88% - Time: 11.64s\n",
      "Epoch [74/100] - Loss: 0.0226 Acc: 99.86% - Time: 11.62s\n",
      "Epoch [75/100] - Loss: 0.0220 Acc: 99.87% - Time: 11.64s\n",
      "Epoch [76/100] - Loss: 0.0212 Acc: 99.89% - Time: 11.70s\n",
      "Epoch [77/100] - Loss: 0.0204 Acc: 99.90% - Time: 11.64s\n",
      "Epoch [78/100] - Loss: 0.0209 Acc: 99.91% - Time: 11.67s\n",
      "Epoch [79/100] - Loss: 0.0205 Acc: 99.90% - Time: 11.64s\n",
      "Epoch [80/100] - Loss: 0.0207 Acc: 99.89% - Time: 11.65s\n",
      "Epoch [81/100] - Loss: 0.0204 Acc: 99.89% - Time: 11.64s\n",
      "Epoch [82/100] - Loss: 0.0204 Acc: 99.90% - Time: 11.67s\n",
      "Epoch [83/100] - Loss: 0.0200 Acc: 99.91% - Time: 11.63s\n",
      "Epoch [84/100] - Loss: 0.0199 Acc: 99.92% - Time: 11.66s\n",
      "Epoch [85/100] - Loss: 0.0196 Acc: 99.92% - Time: 11.67s\n",
      "Epoch [86/100] - Loss: 0.0203 Acc: 99.90% - Time: 11.64s\n",
      "Epoch [87/100] - Loss: 0.0200 Acc: 99.91% - Time: 11.62s\n",
      "Epoch [88/100] - Loss: 0.0199 Acc: 99.93% - Time: 11.65s\n",
      "Epoch [89/100] - Loss: 0.0199 Acc: 99.91% - Time: 11.68s\n",
      "Epoch [90/100] - Loss: 0.0196 Acc: 99.91% - Time: 11.61s\n",
      "Epoch [91/100] - Loss: 0.0196 Acc: 99.91% - Time: 11.64s\n",
      "Epoch [92/100] - Loss: 0.0195 Acc: 99.92% - Time: 11.70s\n",
      "Epoch [93/100] - Loss: 0.0193 Acc: 99.92% - Time: 11.66s\n",
      "Epoch [94/100] - Loss: 0.0195 Acc: 99.92% - Time: 11.53s\n",
      "Epoch [95/100] - Loss: 0.0191 Acc: 99.93% - Time: 11.70s\n",
      "Epoch [96/100] - Loss: 0.0193 Acc: 99.92% - Time: 11.70s\n",
      "Epoch [97/100] - Loss: 0.0189 Acc: 99.93% - Time: 11.65s\n",
      "Epoch [98/100] - Loss: 0.0191 Acc: 99.94% - Time: 11.68s\n",
      "Epoch [99/100] - Loss: 0.0192 Acc: 99.92% - Time: 11.62s\n",
      "Epoch [100/100] - Loss: 0.0189 Acc: 99.93% - Time: 11.64s\n",
      "Training finished!\n",
      "Model saved to resnet18_cifar100_0.pth\n",
      "\n",
      "===== Processing resnet34 =====\n",
      "Training model...\n",
      "Epoch [1/100] - Loss: 3.9726 Acc: 8.68% - Time: 21.20s\n",
      "Epoch [2/100] - Loss: 3.3287 Acc: 18.49% - Time: 21.18s\n",
      "Epoch [3/100] - Loss: 2.8419 Acc: 27.15% - Time: 21.00s\n",
      "Epoch [4/100] - Loss: 2.4282 Acc: 35.83% - Time: 21.15s\n",
      "Epoch [5/100] - Loss: 2.1297 Acc: 42.07% - Time: 21.22s\n",
      "Epoch [6/100] - Loss: 1.8891 Acc: 47.42% - Time: 21.23s\n",
      "Epoch [7/100] - Loss: 1.7114 Acc: 52.16% - Time: 21.22s\n",
      "Epoch [8/100] - Loss: 1.5747 Acc: 55.19% - Time: 21.21s\n",
      "Epoch [9/100] - Loss: 1.4287 Acc: 59.01% - Time: 21.16s\n",
      "Epoch [10/100] - Loss: 1.3387 Acc: 61.08% - Time: 21.20s\n",
      "Epoch [11/100] - Loss: 1.2346 Acc: 63.76% - Time: 21.18s\n",
      "Epoch [12/100] - Loss: 1.1563 Acc: 66.07% - Time: 21.25s\n",
      "Epoch [13/100] - Loss: 1.1059 Acc: 67.26% - Time: 21.19s\n",
      "Epoch [14/100] - Loss: 1.0324 Acc: 69.35% - Time: 21.20s\n",
      "Epoch [15/100] - Loss: 0.9845 Acc: 70.64% - Time: 21.26s\n",
      "Epoch [16/100] - Loss: 0.9245 Acc: 72.14% - Time: 21.20s\n",
      "Epoch [17/100] - Loss: 0.8839 Acc: 73.26% - Time: 21.28s\n",
      "Epoch [18/100] - Loss: 0.8407 Acc: 74.42% - Time: 21.21s\n",
      "Epoch [19/100] - Loss: 0.8023 Acc: 75.73% - Time: 21.25s\n",
      "Epoch [20/100] - Loss: 0.7631 Acc: 76.77% - Time: 21.21s\n",
      "Epoch [21/100] - Loss: 0.7286 Acc: 77.59% - Time: 21.21s\n",
      "Epoch [22/100] - Loss: 0.7057 Acc: 78.31% - Time: 21.29s\n",
      "Epoch [23/100] - Loss: 0.6753 Acc: 79.27% - Time: 21.26s\n",
      "Epoch [24/100] - Loss: 0.6544 Acc: 79.69% - Time: 21.21s\n",
      "Epoch [25/100] - Loss: 0.6286 Acc: 80.26% - Time: 21.19s\n",
      "Epoch [26/100] - Loss: 0.5995 Acc: 81.38% - Time: 21.22s\n",
      "Epoch [27/100] - Loss: 0.5794 Acc: 81.90% - Time: 21.22s\n",
      "Epoch [28/100] - Loss: 0.5637 Acc: 82.56% - Time: 21.15s\n",
      "Epoch [29/100] - Loss: 0.5512 Acc: 82.65% - Time: 21.24s\n",
      "Epoch [30/100] - Loss: 0.5386 Acc: 83.15% - Time: 21.18s\n",
      "Epoch [31/100] - Loss: 0.5132 Acc: 83.94% - Time: 21.22s\n",
      "Epoch [32/100] - Loss: 0.5084 Acc: 84.12% - Time: 21.22s\n",
      "Epoch [33/100] - Loss: 0.4923 Acc: 84.62% - Time: 21.21s\n",
      "Epoch [34/100] - Loss: 0.4813 Acc: 84.94% - Time: 21.20s\n",
      "Epoch [35/100] - Loss: 0.4695 Acc: 85.31% - Time: 21.14s\n",
      "Epoch [36/100] - Loss: 0.4452 Acc: 86.14% - Time: 21.27s\n",
      "Epoch [37/100] - Loss: 0.4571 Acc: 85.57% - Time: 21.16s\n",
      "Epoch [38/100] - Loss: 0.4455 Acc: 86.08% - Time: 20.93s\n",
      "Epoch [39/100] - Loss: 0.4369 Acc: 86.29% - Time: 21.04s\n",
      "Epoch [40/100] - Loss: 0.4109 Acc: 87.07% - Time: 20.96s\n",
      "Epoch [41/100] - Loss: 0.4181 Acc: 86.96% - Time: 21.01s\n",
      "Epoch [42/100] - Loss: 0.4128 Acc: 86.98% - Time: 21.09s\n",
      "Epoch [43/100] - Loss: 0.3951 Acc: 87.66% - Time: 20.91s\n",
      "Epoch [44/100] - Loss: 0.3909 Acc: 87.70% - Time: 20.87s\n",
      "Epoch [45/100] - Loss: 0.4052 Acc: 87.08% - Time: 20.90s\n",
      "Epoch [46/100] - Loss: 0.3856 Acc: 87.94% - Time: 20.95s\n",
      "Epoch [47/100] - Loss: 0.3834 Acc: 87.88% - Time: 20.78s\n",
      "Epoch [48/100] - Loss: 0.3787 Acc: 88.18% - Time: 20.82s\n",
      "Epoch [49/100] - Loss: 0.3785 Acc: 88.12% - Time: 20.73s\n",
      "Epoch [50/100] - Loss: 0.3622 Acc: 88.57% - Time: 20.78s\n",
      "Epoch [51/100] - Loss: 0.1513 Acc: 95.82% - Time: 20.85s\n",
      "Epoch [52/100] - Loss: 0.0835 Acc: 98.11% - Time: 20.76s\n",
      "Epoch [53/100] - Loss: 0.0638 Acc: 98.69% - Time: 20.86s\n",
      "Epoch [54/100] - Loss: 0.0537 Acc: 99.02% - Time: 20.78s\n",
      "Epoch [55/100] - Loss: 0.0465 Acc: 99.16% - Time: 20.71s\n",
      "Epoch [56/100] - Loss: 0.0421 Acc: 99.27% - Time: 20.78s\n",
      "Epoch [57/100] - Loss: 0.0376 Acc: 99.39% - Time: 20.81s\n",
      "Epoch [58/100] - Loss: 0.0342 Acc: 99.47% - Time: 20.77s\n",
      "Epoch [59/100] - Loss: 0.0322 Acc: 99.54% - Time: 20.75s\n",
      "Epoch [60/100] - Loss: 0.0302 Acc: 99.59% - Time: 20.76s\n",
      "Epoch [61/100] - Loss: 0.0278 Acc: 99.64% - Time: 20.95s\n",
      "Epoch [62/100] - Loss: 0.0262 Acc: 99.69% - Time: 20.89s\n",
      "Epoch [63/100] - Loss: 0.0248 Acc: 99.69% - Time: 20.90s\n",
      "Epoch [64/100] - Loss: 0.0231 Acc: 99.78% - Time: 20.82s\n",
      "Epoch [65/100] - Loss: 0.0220 Acc: 99.77% - Time: 20.81s\n",
      "Epoch [66/100] - Loss: 0.0206 Acc: 99.82% - Time: 20.84s\n",
      "Epoch [67/100] - Loss: 0.0202 Acc: 99.83% - Time: 20.82s\n",
      "Epoch [68/100] - Loss: 0.0192 Acc: 99.85% - Time: 20.80s\n",
      "Epoch [69/100] - Loss: 0.0183 Acc: 99.84% - Time: 20.73s\n",
      "Epoch [70/100] - Loss: 0.0177 Acc: 99.86% - Time: 20.77s\n",
      "Epoch [71/100] - Loss: 0.0168 Acc: 99.88% - Time: 20.78s\n",
      "Epoch [72/100] - Loss: 0.0170 Acc: 99.85% - Time: 20.84s\n",
      "Epoch [73/100] - Loss: 0.0161 Acc: 99.89% - Time: 20.81s\n",
      "Epoch [74/100] - Loss: 0.0160 Acc: 99.88% - Time: 20.81s\n",
      "Epoch [75/100] - Loss: 0.0156 Acc: 99.87% - Time: 20.89s\n",
      "Epoch [76/100] - Loss: 0.0148 Acc: 99.91% - Time: 20.74s\n",
      "Epoch [77/100] - Loss: 0.0142 Acc: 99.93% - Time: 20.79s\n",
      "Epoch [78/100] - Loss: 0.0138 Acc: 99.94% - Time: 20.66s\n",
      "Epoch [79/100] - Loss: 0.0139 Acc: 99.92% - Time: 20.92s\n",
      "Epoch [80/100] - Loss: 0.0143 Acc: 99.91% - Time: 20.85s\n",
      "Epoch [81/100] - Loss: 0.0137 Acc: 99.93% - Time: 20.88s\n",
      "Epoch [82/100] - Loss: 0.0137 Acc: 99.91% - Time: 20.80s\n",
      "Epoch [83/100] - Loss: 0.0134 Acc: 99.93% - Time: 20.74s\n",
      "Epoch [84/100] - Loss: 0.0137 Acc: 99.92% - Time: 20.73s\n",
      "Epoch [85/100] - Loss: 0.0137 Acc: 99.93% - Time: 20.72s\n",
      "Epoch [86/100] - Loss: 0.0135 Acc: 99.92% - Time: 20.87s\n",
      "Epoch [87/100] - Loss: 0.0135 Acc: 99.93% - Time: 20.84s\n",
      "Epoch [88/100] - Loss: 0.0133 Acc: 99.92% - Time: 20.96s\n",
      "Epoch [89/100] - Loss: 0.0134 Acc: 99.93% - Time: 20.85s\n",
      "Epoch [90/100] - Loss: 0.0135 Acc: 99.92% - Time: 20.84s\n",
      "Epoch [91/100] - Loss: 0.0134 Acc: 99.92% - Time: 20.82s\n",
      "Epoch [92/100] - Loss: 0.0134 Acc: 99.92% - Time: 20.76s\n",
      "Epoch [93/100] - Loss: 0.0132 Acc: 99.94% - Time: 20.87s\n",
      "Epoch [94/100] - Loss: 0.0134 Acc: 99.92% - Time: 20.99s\n",
      "Epoch [95/100] - Loss: 0.0133 Acc: 99.92% - Time: 21.04s\n",
      "Epoch [96/100] - Loss: 0.0130 Acc: 99.93% - Time: 20.90s\n",
      "Epoch [97/100] - Loss: 0.0130 Acc: 99.93% - Time: 20.99s\n",
      "Epoch [98/100] - Loss: 0.0131 Acc: 99.94% - Time: 21.01s\n",
      "Epoch [99/100] - Loss: 0.0129 Acc: 99.95% - Time: 21.04s\n",
      "Epoch [100/100] - Loss: 0.0132 Acc: 99.95% - Time: 20.90s\n",
      "Training finished!\n",
      "Model saved to resnet34_cifar100_0.pth\n",
      "\n",
      "===== Processing resnet50 =====\n",
      "Training model...\n",
      "Epoch [1/100] - Loss: 4.0170 Acc: 7.81% - Time: 21.01s\n",
      "Epoch [2/100] - Loss: 3.4022 Acc: 17.21% - Time: 21.05s\n",
      "Epoch [3/100] - Loss: 2.9664 Acc: 25.50% - Time: 20.90s\n",
      "Epoch [4/100] - Loss: 2.5739 Acc: 33.24% - Time: 20.96s\n",
      "Epoch [5/100] - Loss: 2.2404 Acc: 39.69% - Time: 20.83s\n",
      "Epoch [6/100] - Loss: 1.9914 Acc: 45.60% - Time: 20.88s\n",
      "Epoch [7/100] - Loss: 1.7956 Acc: 50.03% - Time: 20.83s\n",
      "Epoch [8/100] - Loss: 1.6324 Acc: 54.02% - Time: 20.85s\n",
      "Epoch [9/100] - Loss: 1.4903 Acc: 57.43% - Time: 20.80s\n",
      "Epoch [10/100] - Loss: 1.3811 Acc: 60.28% - Time: 20.84s\n",
      "Epoch [11/100] - Loss: 1.2756 Acc: 62.78% - Time: 20.86s\n",
      "Epoch [12/100] - Loss: 1.1948 Acc: 64.94% - Time: 20.81s\n",
      "Epoch [13/100] - Loss: 1.1323 Acc: 66.70% - Time: 20.82s\n",
      "Epoch [14/100] - Loss: 1.0683 Acc: 68.31% - Time: 20.77s\n",
      "Epoch [15/100] - Loss: 1.0026 Acc: 70.04% - Time: 20.96s\n",
      "Epoch [16/100] - Loss: 0.9487 Acc: 71.63% - Time: 20.88s\n",
      "Epoch [17/100] - Loss: 0.9046 Acc: 72.54% - Time: 20.78s\n",
      "Epoch [18/100] - Loss: 0.8581 Acc: 73.85% - Time: 21.00s\n",
      "Epoch [19/100] - Loss: 0.8263 Acc: 75.01% - Time: 20.89s\n",
      "Epoch [20/100] - Loss: 0.7740 Acc: 76.44% - Time: 20.91s\n",
      "Epoch [21/100] - Loss: 0.7522 Acc: 76.84% - Time: 20.75s\n",
      "Epoch [22/100] - Loss: 0.7166 Acc: 78.00% - Time: 20.76s\n",
      "Epoch [23/100] - Loss: 0.6928 Acc: 78.60% - Time: 20.96s\n",
      "Epoch [24/100] - Loss: 0.6827 Acc: 78.93% - Time: 20.79s\n",
      "Epoch [25/100] - Loss: 0.6537 Acc: 79.71% - Time: 20.82s\n",
      "Epoch [26/100] - Loss: 0.6226 Acc: 80.57% - Time: 20.92s\n",
      "Epoch [27/100] - Loss: 0.6120 Acc: 80.91% - Time: 20.91s\n",
      "Epoch [28/100] - Loss: 0.5903 Acc: 81.71% - Time: 20.85s\n",
      "Epoch [29/100] - Loss: 0.5671 Acc: 82.23% - Time: 20.90s\n",
      "Epoch [30/100] - Loss: 0.5537 Acc: 82.64% - Time: 20.87s\n",
      "Epoch [31/100] - Loss: 0.5423 Acc: 83.16% - Time: 20.97s\n",
      "Epoch [32/100] - Loss: 0.5244 Acc: 83.67% - Time: 20.81s\n",
      "Epoch [33/100] - Loss: 0.5282 Acc: 83.29% - Time: 20.82s\n",
      "Epoch [34/100] - Loss: 0.4962 Acc: 84.47% - Time: 20.80s\n",
      "Epoch [35/100] - Loss: 0.4903 Acc: 84.51% - Time: 20.86s\n",
      "Epoch [36/100] - Loss: 0.4920 Acc: 84.68% - Time: 21.09s\n",
      "Epoch [37/100] - Loss: 0.4706 Acc: 85.24% - Time: 21.12s\n",
      "Epoch [38/100] - Loss: 0.4718 Acc: 85.09% - Time: 21.08s\n",
      "Epoch [39/100] - Loss: 0.4505 Acc: 85.77% - Time: 20.95s\n",
      "Epoch [40/100] - Loss: 0.4422 Acc: 86.29% - Time: 20.85s\n",
      "Epoch [41/100] - Loss: 0.4481 Acc: 85.93% - Time: 20.90s\n",
      "Epoch [42/100] - Loss: 0.4287 Acc: 86.43% - Time: 20.94s\n",
      "Epoch [43/100] - Loss: 0.4236 Acc: 86.63% - Time: 20.92s\n",
      "Epoch [44/100] - Loss: 0.4171 Acc: 86.88% - Time: 20.97s\n",
      "Epoch [45/100] - Loss: 0.4182 Acc: 86.91% - Time: 20.74s\n",
      "Epoch [46/100] - Loss: 0.4022 Acc: 87.46% - Time: 20.74s\n",
      "Epoch [47/100] - Loss: 0.4098 Acc: 87.14% - Time: 20.81s\n",
      "Epoch [48/100] - Loss: 0.3832 Acc: 87.85% - Time: 20.76s\n",
      "Epoch [49/100] - Loss: 0.3824 Acc: 87.96% - Time: 20.81s\n",
      "Epoch [50/100] - Loss: 0.3918 Acc: 87.80% - Time: 20.85s\n",
      "Epoch [51/100] - Loss: 0.1632 Acc: 95.48% - Time: 20.99s\n",
      "Epoch [52/100] - Loss: 0.0886 Acc: 98.01% - Time: 21.14s\n",
      "Epoch [53/100] - Loss: 0.0673 Acc: 98.66% - Time: 21.16s\n",
      "Epoch [54/100] - Loss: 0.0561 Acc: 98.89% - Time: 21.17s\n",
      "Epoch [55/100] - Loss: 0.0491 Acc: 99.09% - Time: 20.94s\n",
      "Epoch [56/100] - Loss: 0.0441 Acc: 99.28% - Time: 21.11s\n",
      "Epoch [57/100] - Loss: 0.0398 Acc: 99.32% - Time: 21.15s\n",
      "Epoch [58/100] - Loss: 0.0359 Acc: 99.38% - Time: 21.09s\n",
      "Epoch [59/100] - Loss: 0.0331 Acc: 99.50% - Time: 21.12s\n",
      "Epoch [60/100] - Loss: 0.0307 Acc: 99.55% - Time: 21.09s\n",
      "Epoch [61/100] - Loss: 0.0275 Acc: 99.62% - Time: 21.10s\n",
      "Epoch [62/100] - Loss: 0.0260 Acc: 99.65% - Time: 21.01s\n",
      "Epoch [63/100] - Loss: 0.0248 Acc: 99.68% - Time: 21.10s\n",
      "Epoch [64/100] - Loss: 0.0242 Acc: 99.65% - Time: 21.13s\n",
      "Epoch [65/100] - Loss: 0.0227 Acc: 99.70% - Time: 21.15s\n",
      "Epoch [66/100] - Loss: 0.0212 Acc: 99.78% - Time: 21.11s\n",
      "Epoch [67/100] - Loss: 0.0200 Acc: 99.81% - Time: 21.12s\n",
      "Epoch [68/100] - Loss: 0.0191 Acc: 99.83% - Time: 21.11s\n",
      "Epoch [69/100] - Loss: 0.0189 Acc: 99.83% - Time: 21.14s\n",
      "Epoch [70/100] - Loss: 0.0180 Acc: 99.84% - Time: 21.09s\n",
      "Epoch [71/100] - Loss: 0.0175 Acc: 99.83% - Time: 21.15s\n",
      "Epoch [72/100] - Loss: 0.0172 Acc: 99.87% - Time: 21.17s\n",
      "Epoch [73/100] - Loss: 0.0166 Acc: 99.85% - Time: 21.16s\n",
      "Epoch [74/100] - Loss: 0.0162 Acc: 99.88% - Time: 21.14s\n",
      "Epoch [75/100] - Loss: 0.0153 Acc: 99.88% - Time: 21.15s\n",
      "Epoch [76/100] - Loss: 0.0146 Acc: 99.90% - Time: 21.09s\n",
      "Epoch [77/100] - Loss: 0.0137 Acc: 99.91% - Time: 21.08s\n",
      "Epoch [78/100] - Loss: 0.0138 Acc: 99.92% - Time: 21.09s\n",
      "Epoch [79/100] - Loss: 0.0139 Acc: 99.90% - Time: 21.12s\n",
      "Epoch [80/100] - Loss: 0.0134 Acc: 99.93% - Time: 21.14s\n",
      "Epoch [81/100] - Loss: 0.0137 Acc: 99.92% - Time: 21.11s\n",
      "Epoch [82/100] - Loss: 0.0137 Acc: 99.91% - Time: 21.13s\n",
      "Epoch [83/100] - Loss: 0.0134 Acc: 99.92% - Time: 21.14s\n",
      "Epoch [84/100] - Loss: 0.0136 Acc: 99.92% - Time: 21.16s\n",
      "Epoch [85/100] - Loss: 0.0133 Acc: 99.92% - Time: 21.20s\n",
      "Epoch [86/100] - Loss: 0.0132 Acc: 99.92% - Time: 21.11s\n",
      "Epoch [87/100] - Loss: 0.0138 Acc: 99.89% - Time: 21.21s\n",
      "Epoch [88/100] - Loss: 0.0130 Acc: 99.92% - Time: 21.16s\n",
      "Epoch [89/100] - Loss: 0.0132 Acc: 99.92% - Time: 21.15s\n",
      "Epoch [90/100] - Loss: 0.0131 Acc: 99.91% - Time: 21.10s\n",
      "Epoch [91/100] - Loss: 0.0130 Acc: 99.93% - Time: 21.25s\n",
      "Epoch [92/100] - Loss: 0.0126 Acc: 99.93% - Time: 21.15s\n",
      "Epoch [93/100] - Loss: 0.0129 Acc: 99.93% - Time: 21.09s\n",
      "Epoch [94/100] - Loss: 0.0128 Acc: 99.93% - Time: 21.10s\n",
      "Epoch [95/100] - Loss: 0.0128 Acc: 99.93% - Time: 21.08s\n",
      "Epoch [96/100] - Loss: 0.0124 Acc: 99.95% - Time: 21.07s\n",
      "Epoch [97/100] - Loss: 0.0129 Acc: 99.94% - Time: 21.09s\n",
      "Epoch [98/100] - Loss: 0.0123 Acc: 99.94% - Time: 21.02s\n",
      "Epoch [99/100] - Loss: 0.0126 Acc: 99.94% - Time: 21.14s\n",
      "Epoch [100/100] - Loss: 0.0125 Acc: 99.93% - Time: 21.08s\n",
      "Training finished!\n",
      "Model saved to resnet50_cifar100_0.pth\n"
     ]
    }
   ],
   "source": [
    "DATASETS = ['cifar10', 'cifar100']\n",
    "MODELS = ['resnet18','resnet34','resnet50'] \n",
    "METHODS = ['likelihood','cumulative']\n",
    "EPOCHS = 100\n",
    "CALIB_SIZE = 1000\n",
    "TEST_SIZE = 1000 \n",
    "BATCH_SIZE = 128\n",
    "\n",
    "for DATASET in DATASETS:\n",
    "\n",
    "    print(\"Loading data...\")\n",
    "    data = CIFARData(dataset=DATASET, calib_size=CALIB_SIZE, test_size=TEST_SIZE, batch_size=BATCH_SIZE)\n",
    "    num_classes = data.num_classes\n",
    "    np.save(f'{DATASET}_split_indices_0.npy', data.split_indices)\n",
    "\n",
    "    for model_name in MODELS:\n",
    "        print(f\"\\n===== Processing {model_name} =====\")\n",
    "        \n",
    "        if model_name == 'resnet18':\n",
    "            model = ResNet18(num_classes)\n",
    "        elif model_name == 'resnet34':\n",
    "            model = ResNet34(num_classes)\n",
    "        else:  # resnet50\n",
    "            model = ResNet50(num_classes)\n",
    "        \n",
    "        print(\"Training model...\")\n",
    "        trainer = Trainer(model, data, num_classes=num_classes, epochs=EPOCHS)\n",
    "        model = trainer.train()\n",
    "        \n",
    "        model_path = f\"{model_name}_{DATASET}_0.pth\"\n",
    "        trainer.save_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb607174",
   "metadata": {},
   "source": [
    "二 做一些测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edb3ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Processing model: resnet18\n",
      "==================================================\n",
      "Dataset: cifar10\n",
      "\n",
      "Dataset sizes for CIFAR10:\n",
      "  Training set: 49000 samples\n",
      "  Calibration set: 1000 samples\n",
      "  Test set: 10000 samples\n",
      "Created calibration loader for cifar10\n",
      "  Calibration set size: 1000 samples\n",
      "  Batch size: 128\n",
      "Loaded state_dict from resnet18_cifar10_0.pth\n",
      "\n",
      "Calibrating model...\n",
      "Calibration method: likelihood\n",
      "Scores statistics (likelihood):\n",
      "  Min: 0.000001, Max: 0.999972\n",
      "  Mean: 0.065806, Median: 0.000448\n",
      "  10th percentile: 0.000065\n",
      "  90th percentile: 0.119974\n",
      "Calibration complete - Quantile: 0.9297\n",
      "\n",
      "Evaluating model on test set...\n",
      "Prediction sets saved to: resnet18_cifar10_likelihood_prediction_set_0.csv\n",
      "Coverage: 97.21% | Avg Set Size: 1.09 | Accuracy: 94.56% | Avg Loss: 0.2108\n",
      "Full results saved to resnet18_cifar10_likelihood_full_results_0.pkl\n",
      "\n",
      "Calibrating model...\n",
      "Calibration method: cumulative\n",
      "Scores statistics (cumulative):\n",
      "  Min: 0.460271, Max: 1.000000\n",
      "  Mean: 0.996258, Median: 0.999954\n",
      "  10th percentile: 0.999002\n",
      "  90th percentile: 1.000000\n",
      "Calibration complete - Quantile: 1.0000\n",
      "\n",
      "Evaluating model on test set...\n",
      "Prediction sets saved to: resnet18_cifar10_cumulative_prediction_set_0.csv\n",
      "Coverage: 94.56% | Avg Set Size: 1.00 | Accuracy: 94.56% | Avg Loss: 0.2108\n",
      "Full results saved to resnet18_cifar10_cumulative_full_results_0.pkl\n",
      "\n",
      "==================================================\n",
      "Processing model: resnet34\n",
      "==================================================\n",
      "Dataset: cifar10\n",
      "\n",
      "Dataset sizes for CIFAR10:\n",
      "  Training set: 49000 samples\n",
      "  Calibration set: 1000 samples\n",
      "  Test set: 10000 samples\n",
      "Created calibration loader for cifar10\n",
      "  Calibration set size: 1000 samples\n",
      "  Batch size: 128\n",
      "Loaded state_dict from resnet34_cifar10_0.pth\n",
      "\n",
      "Calibrating model...\n",
      "Calibration method: likelihood\n",
      "Scores statistics (likelihood):\n",
      "  Min: 0.000001, Max: 0.999997\n",
      "  Mean: 0.056579, Median: 0.000138\n",
      "  10th percentile: 0.000020\n",
      "  90th percentile: 0.019005\n",
      "Calibration complete - Quantile: 0.9829\n",
      "\n",
      "Evaluating model on test set...\n",
      "Prediction sets saved to: resnet34_cifar10_likelihood_prediction_set_0.csv\n",
      "Coverage: 97.72% | Avg Set Size: 1.12 | Accuracy: 94.84% | Avg Loss: 0.2316\n",
      "Full results saved to resnet34_cifar10_likelihood_full_results_0.pkl\n",
      "\n",
      "Calibrating model...\n",
      "Calibration method: cumulative\n",
      "Scores statistics (cumulative):\n",
      "  Min: 0.653023, Max: 1.000000\n",
      "  Mean: 0.998002, Median: 0.999987\n",
      "  10th percentile: 0.999684\n",
      "  90th percentile: 1.000000\n",
      "Calibration complete - Quantile: 1.0000\n",
      "\n",
      "Evaluating model on test set...\n",
      "Prediction sets saved to: resnet34_cifar10_cumulative_prediction_set_0.csv\n",
      "Coverage: 94.84% | Avg Set Size: 1.00 | Accuracy: 94.84% | Avg Loss: 0.2316\n",
      "Full results saved to resnet34_cifar10_cumulative_full_results_0.pkl\n",
      "\n",
      "==================================================\n",
      "Processing model: resnet50\n",
      "==================================================\n",
      "Dataset: cifar10\n",
      "\n",
      "Dataset sizes for CIFAR10:\n",
      "  Training set: 49000 samples\n",
      "  Calibration set: 1000 samples\n",
      "  Test set: 10000 samples\n",
      "Created calibration loader for cifar10\n",
      "  Calibration set size: 1000 samples\n",
      "  Batch size: 128\n",
      "Loaded state_dict from resnet50_cifar10_0.pth\n",
      "\n",
      "Calibrating model...\n",
      "Calibration method: likelihood\n",
      "Scores statistics (likelihood):\n",
      "  Min: 0.000000, Max: 0.999998\n",
      "  Mean: 0.051241, Median: 0.000102\n",
      "  10th percentile: 0.000010\n",
      "  90th percentile: 0.017213\n",
      "Calibration complete - Quantile: 0.8706\n",
      "\n",
      "Evaluating model on test set...\n",
      "Prediction sets saved to: resnet50_cifar10_likelihood_prediction_set_0.csv\n",
      "Coverage: 96.10% | Avg Set Size: 1.05 | Accuracy: 94.53% | Avg Loss: 0.2395\n",
      "Full results saved to resnet50_cifar10_likelihood_full_results_0.pkl\n",
      "\n",
      "Calibrating model...\n",
      "Calibration method: cumulative\n",
      "Scores statistics (cumulative):\n",
      "  Min: 0.530543, Max: 1.000000\n",
      "  Mean: 0.997580, Median: 0.999992\n",
      "  10th percentile: 0.999743\n",
      "  90th percentile: 1.000000\n",
      "Calibration complete - Quantile: 1.0000\n",
      "\n",
      "Evaluating model on test set...\n",
      "Prediction sets saved to: resnet50_cifar10_cumulative_prediction_set_0.csv\n",
      "Coverage: 94.53% | Avg Set Size: 1.00 | Accuracy: 94.53% | Avg Loss: 0.2395\n",
      "Full results saved to resnet50_cifar10_cumulative_full_results_0.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "CALIB_SIZE = 1000\n",
    "TEST_SIZE = 1000 \n",
    "BATCH_SIZE = 128\n",
    "\n",
    "DATASETS = ['cifar10', 'cifar100']\n",
    "MODELS = ['resnet18', 'resnet34', 'resnet50']\n",
    "METHODS = ['likelihood', 'cumulative']\n",
    "\n",
    "DATASET = 'cifar10' # 'cifar100'\n",
    "\n",
    "def create_calib_loader(dataset_name, split_indices_file, batch_size=128):\n",
    "    \"\"\"\n",
    "    Create a calibration data loader based on the saved index file\n",
    "\n",
    "    Parameters:\n",
    "        dataset_name: Name of the dataset ('cifar10' or 'cifar100')\n",
    "        split_indices_file: Path to the saved index file\n",
    "        batch_size: Batch size of the data loader\n",
    "\n",
    "    Returns:\n",
    "    calib_loader: Calibration data loader\n",
    "    \"\"\"\n",
    "    split_indices = np.load(split_indices_file, allow_pickle=True).item()\n",
    "    calib_idx = split_indices['calib_idx']\n",
    "    \n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "    \n",
    "    if dataset_name == 'cifar10':\n",
    "        full_train_set = torchvision.datasets.CIFAR10(\n",
    "            root='./data', train=True, download=True, transform=transform_test)\n",
    "    else:  # cifar100\n",
    "        full_train_set = torchvision.datasets.CIFAR100(\n",
    "            root='./data', train=True, download=True, transform=transform_test)\n",
    "    \n",
    "    calib_set = Subset(full_train_set, calib_idx)\n",
    "    \n",
    "    calib_loader = DataLoader(\n",
    "        calib_set, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=2\n",
    "    )\n",
    "    \n",
    "    print(f\"Created calibration loader for {dataset_name}\")\n",
    "    print(f\"  Calibration set size: {len(calib_set)} samples\")\n",
    "    print(f\"  Batch size: {batch_size}\")\n",
    "    \n",
    "    return calib_loader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for model_name in MODELS:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Processing model: {model_name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "\n",
    "    data = CIFARData(dataset=DATASET, calib_size=CALIB_SIZE, test_size=TEST_SIZE, batch_size=BATCH_SIZE)\n",
    "    num_classes = data.num_classes\n",
    "\n",
    "    model_path = f\"{model_name}_{DATASET}_0.pth\"\n",
    "    split_indices_file = f'{DATASET}_split_indices_0.npy'\n",
    "    \n",
    "    calib_loader = create_calib_loader(DATASET, split_indices_file)\n",
    "    test_loader = data.test_loader\n",
    "\n",
    "    model_path = f\"{model_name}_{DATASET}_0.pth\"\n",
    "\n",
    "    if model_name == 'resnet18':\n",
    "        model = ResNet18(num_classes=num_classes)\n",
    "    elif model_name == 'resnet34':\n",
    "        model = ResNet34(num_classes=num_classes)\n",
    "    elif model_name == 'resnet50':\n",
    "        model = ResNet50(num_classes=num_classes)\n",
    "\n",
    "    state_dict = torch.load(model_path, weights_only=False)\n",
    "    model.load_state_dict(state_dict)\n",
    "    print(f\"Loaded state_dict from {model_path}\")\n",
    "\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    for method in METHODS:\n",
    "        cp = ConformalPredictor(model, calib_loader=calib_loader, num_classes=10)\n",
    "        \n",
    "        print(\"\\nCalibrating model...\")\n",
    "        cp.calibrate(method=method)\n",
    "        \n",
    "        print(\"\\nEvaluating model on test set...\")\n",
    "        save_path = f\"{model_name}_{DATASET}_{method}_prediction_set_0.csv\"\n",
    "        results = cp.evaluate(test_loader, method=method, save_path=save_path)\n",
    "        \n",
    "        results_path = f\"{model_name}_{DATASET}_{method}_full_results_0.pkl\"\n",
    "        with open(results_path, 'wb') as f:\n",
    "            pickle.dump(results, f)\n",
    "        print(f\"Full results saved to {results_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
